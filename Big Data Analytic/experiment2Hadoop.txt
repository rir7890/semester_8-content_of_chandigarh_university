

C:\hadoop\sbin>jps
19368 Jps

C:\hadoop\sbin>start-dfs.cmd

C:\hadoop\sbin>start-yarn.cmd
starting yarn daemons

C:\hadoop\sbin>jps
1444 NameNode
18564 DataNode
17768 Jps
8 NodeManager
19708 ResourceManager

C:\hadoop\sbin>hadoop fs -mkdir /input

C:\hadoop\sbin>hadoop fs -put C:/data.txt /input

C:\hadoop\sbin>hadoop fs -ls /input/
Found 1 items
-rw-r--r--   1 prasa supergroup         69 2024-02-17 15:48 /input/data.txt

C:\hadoop\sbin>hadoop dfs -cat /input/data.txt
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
Hello
Hi
Hello
hi
good
morning
good day
bye
hello
hello
Hi
C:\hadoop\sbin>hadoop jar C:/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4
JAR does not exist or is not a normal file: C:\hadoop\share\hadoop\mapreduce\hadoop-mapreduce-examples-3.2.4

C:\hadoop\sbin>hadoop jar C:/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar
An example program must be given as the first argument.
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
  dbcount: An example job that count the pageview counts from a database.
  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  secondarysort: An example defining a secondary sort to the reduce.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  teragen: Generate data for the terasort
  terasort: Run the terasort
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
  wordmedian: A map/reduce program that counts the median length of the words in the input files.
  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.

C:\hadoop\sbin>hadoop jar C:/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar wordcount/input /out
Unknown program 'wordcount/input' chosen.
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
  dbcount: An example job that count the pageview counts from a database.
  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  secondarysort: An example defining a secondary sort to the reduce.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  teragen: Generate data for the terasort
  terasort: Run the terasort
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
  wordmedian: A map/reduce program that counts the median length of the words in the input files.
  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.

C:\hadoop\sbin>hadoop jar C:/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar wordcount /input /out
2024-02-17 15:55:38,466 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2024-02-17 15:55:40,377 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/prasa/.staging/job_1708164736771_0001
2024-02-17 15:55:41,219 INFO input.FileInputFormat: Total input files to process : 1
2024-02-17 15:55:41,346 INFO mapreduce.JobSubmitter: number of splits:1
2024-02-17 15:55:41,753 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1708164736771_0001
2024-02-17 15:55:41,756 INFO mapreduce.JobSubmitter: Executing with tokens: []
2024-02-17 15:55:42,138 INFO conf.Configuration: resource-types.xml not found
2024-02-17 15:55:42,140 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2024-02-17 15:55:42,953 INFO impl.YarnClientImpl: Submitted application application_1708164736771_0001
2024-02-17 15:55:43,036 INFO mapreduce.Job: The url to track the job: http://rahulkumar:8088/proxy/application_1708164736771_0001/
2024-02-17 15:55:43,038 INFO mapreduce.Job: Running job: job_1708164736771_0001
2024-02-17 15:56:03,548 INFO mapreduce.Job: Job job_1708164736771_0001 running in uber mode : false
2024-02-17 15:56:03,552 INFO mapreduce.Job:  map 0% reduce 0%
2024-02-17 15:56:14,970 INFO mapreduce.Job:  map 100% reduce 0%
2024-02-17 15:56:27,151 INFO mapreduce.Job:  map 100% reduce 100%
2024-02-17 15:56:28,181 INFO mapreduce.Job: Job job_1708164736771_0001 completed successfully
2024-02-17 15:56:28,557 INFO mapreduce.Job: Counters: 54
        File System Counters
                FILE: Number of bytes read=93
                FILE: Number of bytes written=478309
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=170
                HDFS: Number of bytes written=55
                HDFS: Number of read operations=8
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
                HDFS: Number of bytes read erasure-coded=0
        Job Counters
                Launched map tasks=1
                Launched reduce tasks=1
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=8669
                Total time spent by all reduces in occupied slots (ms)=8980
                Total time spent by all map tasks (ms)=8669
                Total time spent by all reduce tasks (ms)=8980
                Total vcore-milliseconds taken by all map tasks=8669
                Total vcore-milliseconds taken by all reduce tasks=8980
                Total megabyte-milliseconds taken by all map tasks=8877056
                Total megabyte-milliseconds taken by all reduce tasks=9195520
        Map-Reduce Framework
                Map input records=11
                Map output records=12
                Map output bytes=107
                Map output materialized bytes=93
                Input split bytes=101
                Combine input records=12
                Combine output records=8
                Reduce input groups=8
                Reduce shuffle bytes=93
                Reduce input records=8
                Reduce output records=8
                Spilled Records=16
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=163
                CPU time spent (ms)=1529
                Physical memory (bytes) snapshot=599601152
                Virtual memory (bytes) snapshot=978411520
                Total committed heap usage (bytes)=503316480
                Peak Map Physical memory (bytes)=352772096
                Peak Map Virtual memory (bytes)=489213952
                Peak Reduce Physical memory (bytes)=246829056
                Peak Reduce Virtual memory (bytes)=489197568
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=69
        File Output Format Counters
                Bytes Written=55

C:\hadoop\sbin>hadoop fs -cat /out/*
Hello   2
Hi      2
bye     1
day     1
good    2
hello   2
hi      1
morning 1

C:\hadoop\sbin>